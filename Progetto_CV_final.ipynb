{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Progetto_CV_final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "hB2bp_6alvJv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvdkfSbY9zG-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision.io import read_image\n",
        "import pdb\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from PIL import ImageFile\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "metadata": {
        "id": "9954GPs6xfO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp=torch.randn([2,3,224,224])"
      ],
      "metadata": {
        "id": "BxaF4fJN_Rj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QzCXyCT9RSl",
        "outputId": "41a4e925-a318-40f8-a190-9ee2918d2cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping\n"
      ],
      "metadata": {
        "id": "iJd9PVJvswCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serengeti Dataset"
      ],
      "metadata": {
        "id": "4HfLAX8qti43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SerengetiDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        specie = self.img_labels.iloc[idx, 1]\n",
        "        descrizione = self.img_labels.iloc[idx, 2:5]\n",
        "        descrizione = descrizione.to_list()\n",
        "        descrizione = torch.tensor(descrizione, dtype=torch.float)\n",
        "        emptyimg = self.img_labels.iloc[idx, 5]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            specie = self.target_transform(specie)\n",
        "            descrizione = self.target_transform(descrizione)\n",
        "            emptyimg = self.target_transform(emptyimg)\n",
        "\n",
        "        return image, specie, descrizione, emptyimg"
      ],
      "metadata": {
        "id": "_X3teIwGtn0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print, loss_based=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.val_acc_max = 0.0\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "        self.loss_based = loss_based\n",
        "\n",
        "    def __call__(self, val_score, model):\n",
        "        if self.loss_based:\n",
        "            score = -val_score\n",
        "        else:\n",
        "            score = val_score\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_score, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_score, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_score, model):\n",
        "        # Saves model when better val_score found\n",
        "        if self.verbose:\n",
        "            self.trace_func(\n",
        "                f'Validation loss decreased ({self.val_loss_min:.4f} --> {val_score:.4f}).  Saving model ...'\n",
        "                if self.loss_based else f'Validation Accuracy increased ({self.val_acc_max:.4f} --> '\n",
        "                                        f'{val_score:.4f}).  'f'Saving model ...')\n",
        "            print()\n",
        "\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "\n",
        "        if self.loss_based:\n",
        "            self.val_loss_min = val_score\n",
        "        else:\n",
        "            self.val_acc_max = val_score\n",
        "\n",
        "    def reset (self):\n",
        "        self.best_score=None"
      ],
      "metadata": {
        "id": "JnB1EtAPs22V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classi per blocks"
      ],
      "metadata": {
        "id": "5NviPBdAlwSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion=1\n",
        "\n",
        "  def __init__(self,inplanes,planes,stride=1,downsample=None):\n",
        "      super().__init__()\n",
        "      self.conv1=nn.Conv2d(inplanes,planes,kernel_size=3,stride=stride,padding=1,bias=False)\n",
        "      self.bn1 = nn.BatchNorm2d(planes)\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
        "                     padding=1, bias=False)\n",
        "      self.bn2 = nn.BatchNorm2d(planes)\n",
        "      self.downsample = downsample\n",
        "      self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "      identity = x\n",
        "\n",
        "      out = self.conv1(x)\n",
        "      out = self.bn1(out)\n",
        "      out = self.relu(out)\n",
        "\n",
        "      out = self.conv2(out)\n",
        "      out = self.bn2(out)\n",
        "\n",
        "      if self.downsample is not None:\n",
        "          identity = self.downsample(x)\n",
        "\n",
        "      out += identity\n",
        "      out = self.relu(out)\n",
        "\n",
        "      return out"
      ],
      "metadata": {
        "id": "LwFG6GY-In5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ],
      "metadata": {
        "id": "t79N-3gN7kNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        #self.conv1= nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "rqITCoPo237T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classi Branch"
      ],
      "metadata": {
        "id": "KZn0OlB_t_Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationBranch(nn.Module):\n",
        "  def __init__(self,inplanes,planes):\n",
        "      super(ClassificationBranch,self).__init__()\n",
        "      self.conv1=nn.Conv2d(inplanes,planes,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "      self.bn1=nn.BatchNorm2d(planes)\n",
        "      self.relu1=nn.ReLU(inplace=True)\n",
        "      self.pool1=nn.AdaptiveMaxPool2d((1,1))\n",
        "      self.linear1=nn.Linear(in_features=planes,out_features=64)\n",
        "      self.bn2 = nn.BatchNorm1d(num_features=64)\n",
        "      self.relu2=nn.ReLU(inplace=True)\n",
        "      self.dropout1=nn.Dropout(0.2)\n",
        "      self.linear2 = nn.Linear(in_features=64, out_features=32)\n",
        "      self.bn3=nn.BatchNorm1d(num_features=32)\n",
        "      self.relu3=nn.ReLU(inplace=True)\n",
        "      self.dropout2=nn.Dropout(0.2)\n",
        "      self.lineaer3=nn.Linear(in_features=32,out_features=2)\n",
        "      \n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.conv1(x)\n",
        "    x=self.bn1(x)\n",
        "    x=self.relu1(x)\n",
        "    x=self.pool1(x)\n",
        "    x=torch.flatten(x,1)\n",
        "    x=self.linear1(x)\n",
        "    x=self.bn2(x)\n",
        "    x=self.relu2(x)\n",
        "    x=self.dropout1(x)\n",
        "    x=self.linear2(x)\n",
        "    x=self.bn3(x)\n",
        "    x=self.relu3(x)\n",
        "    x=self.dropout2(x)\n",
        "    x=self.lineaer3(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qn1PuYDc2LMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LightClassificationBranch(nn.Module):\n",
        "  def __init__(self,inplanes,num_classes):\n",
        "      super(LightClassificationBranch,self).__init__()\n",
        "      self.conv1=nn.Conv2d(inplanes,512,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "      self.bn1=nn.BatchNorm2d(512)\n",
        "      self.relu1=nn.ReLU(inplace=True)\n",
        "      \n",
        "      self.conv2=nn.Conv2d(512,2048,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "      self.bn2=nn.BatchNorm2d(2048)\n",
        "      self.relu2=nn.ReLU(inplace=True)\n",
        "      self.pool=nn.AdaptiveMaxPool2d((1,1))\n",
        "      \n",
        "      self.linear1=nn.Linear(in_features=2048,out_features=num_classes)\n",
        "      \n",
        "      \n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.conv1(x)\n",
        "    x=self.bn1(x)\n",
        "    x=self.relu1(x)\n",
        "    x=self.conv2(x)\n",
        "    x=self.bn2(x)\n",
        "    x=self.relu2(x)\n",
        "    x=self.pool(x)\n",
        "    \n",
        "    x=torch.flatten(x,1)\n",
        "    out=self.linear1(x)\n",
        "    \n",
        "    \n",
        "    return x,out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pyB8gwBf2Jl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backbone34"
      ],
      "metadata": {
        "id": "fDbbtHYN15VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone34withoutBranches"
      ],
      "metadata": {
        "id": "KqXsxqDjI3og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Backbone34(nn.Module):\n",
        "  def __init__(self,block,block2,layers,num_classes=21):\n",
        "      super().__init__()\n",
        "\n",
        "      self.inplanes=64\n",
        "      self.conv1=nn.Conv2d(3,self.inplanes,kernel_size=7,stride=2,padding=3,bias=False)\n",
        "      self.bn1=nn.BatchNorm2d(self.inplanes)\n",
        "      self.relu=nn.ReLU(inplace=True)\n",
        "      self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "      #self.classificationBranch=ClassificationBranch(inplanes=64,planes=128)\n",
        "      \n",
        "      self.layer1=self._make_layer(block,64,layers[0])\n",
        "      #self.LightClassificationBranch1=LightClassificationBranch(64,num_classes=num_classes)\n",
        "      \n",
        "      self.layer2=self._make_layer(block,128,layers[1],stride=2)\n",
        "      #self.LightClassificationBranch2=LightClassificationBranch(128,num_classes=num_classes)\n",
        "      self.layer3=self._make_layer(block,256,1,stride=2)\n",
        "\n",
        "      self.layer32=self._make_layer(block,256,layers[2])\n",
        "      self.layer42=self._make_layer(block,512,layers[3],stride=2)\n",
        "      self.avgpool2=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc2=nn.Linear(512*block.expansion,3)\n",
        "      \n",
        "      self.inplanes=256\n",
        "      self.layer31=self._make_layer(block2,128,layers[4],stride=2)\n",
        "      self.layer41=self._make_layer(block2,256,layers[5],stride=2)\n",
        "      self.layer51=self._make_layer(block2,512,layers[6],stride=2)\n",
        "      \n",
        "\n",
        "     \n",
        "      self.avgpool1=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc1=nn.Linear(512*block2.expansion,num_classes)\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "      downsample = None  \n",
        "  \n",
        "      if stride != 1 or self.inplanes != planes*block.expansion:\n",
        "          downsample = nn.Sequential(\n",
        "              nn.Conv2d(self.inplanes, planes*block.expansion, 1, stride, bias=False),\n",
        "              nn.BatchNorm2d(planes*block.expansion),\n",
        "          )\n",
        "\n",
        "      layers = []\n",
        "      layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "      \n",
        "      self.inplanes = planes*block.expansion\n",
        "      \n",
        "      for _ in range(1, blocks):\n",
        "          layers.append(block(self.inplanes, planes))\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=self.conv1(x)\n",
        "      x=self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.maxpool(x)         # 112x112\n",
        "      \n",
        "      x = self.layer1(x)  \n",
        "             \n",
        "      \n",
        "      x = self.layer2(x) \n",
        "            \n",
        "      x = self.layer3(x)          # 14x14\n",
        "      \n",
        "      x1 = self.layer31(x)\n",
        "      x1= self.layer41(x1)          # 7x7\n",
        "      x1=self.layer51(x1)\n",
        "      x1 = self.avgpool1(x1)         # 1x1\n",
        "      x1 = torch.flatten(x1, 1)     # remove 1 X 1 grid and make vector of tensor shape \n",
        "      x1 = self.fc1(x1)\n",
        "      \n",
        "      x2= self.layer32(x)\n",
        "      x2=self.layer42(x2)\n",
        "      x2=self.avgpool2(x2)\n",
        "      x2=torch.flatten(x2,1)\n",
        "      x2=self.fc2(x2)\n",
        "\n",
        "\n",
        "      return x1,x2"
      ],
      "metadata": {
        "id": "4LEBhLj9PpRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone34=backbone34()\n",
        "backbone34"
      ],
      "metadata": {
        "id": "SjrRabORQUYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1,x2=backbone34(inp)"
      ],
      "metadata": {
        "id": "O84homRkQarH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone34(inp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1ozujx4UnUN",
        "outputId": "f4685b7d-d059-48ad-c2db-e1c63c76254e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.4741, -0.1395, -0.0648, -0.3945,  0.9902, -0.8282, -0.5500,  1.0429,\n",
              "          -0.6244, -0.0940, -0.1082,  1.0672, -0.3699, -1.0673,  0.3096, -0.1033,\n",
              "          -0.1673,  0.5847, -0.2893, -2.4647, -0.7236],\n",
              "         [ 0.3921,  0.1848,  0.5467,  0.2984,  0.6095, -1.1468,  0.2949,  0.5634,\n",
              "          -0.7786, -0.4431, -1.1236,  1.4900, -0.3843, -0.9994,  0.3966,  0.3101,\n",
              "          -0.4375,  0.7542, -0.9208, -1.7277, -1.5801]],\n",
              "        grad_fn=<AddmmBackward0>), tensor([[ 0.1025,  0.3287, -0.5659],\n",
              "         [ 0.1137,  0.4606, -0.4706]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShS3d37-QeuN",
        "outputId": "f816cc7e-ee84-4d87-9fac-c79baf26c468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 21])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initializeWeights34(backbone34)"
      ],
      "metadata": {
        "id": "Klw4Mr6wdpXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=backbone34.conv1.weight\n",
        "b=resnetpre.conv1.weight\n",
        "torch.equal(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2pgLjLVdXLP",
        "outputId": "55b59c49-76eb-4e67-c4e0-dccb74bea231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone34withAddBranches"
      ],
      "metadata": {
        "id": "RRhEJ0oYItSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backbone34withAddBranche=backbone34withAddBranches()\n"
      ],
      "metadata": {
        "id": "pUntigXUJStp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out1,out2,x1,x2= backbone34withAddBranche(inp)\n"
      ],
      "metadata": {
        "id": "GaEHV5H6CZ89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1eZ7iKDEUFr",
        "outputId": "430fd145-41a3-43d1-9950-897de4d56fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetModmodAdd34(nn.Module):\n",
        "  def __init__(self,block,block2,layers,num_classes=21):\n",
        "      super().__init__()\n",
        "\n",
        "      self.inplanes=64\n",
        "      self.conv1=nn.Conv2d(3,self.inplanes,kernel_size=7,stride=2,padding=3,bias=False)\n",
        "      self.bn1=nn.BatchNorm2d(self.inplanes)\n",
        "      self.relu=nn.ReLU(inplace=True)\n",
        "      self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "      #self.classificationBranch=ClassificationBranch(inplanes=64,planes=128)\n",
        "      \n",
        "      self.layer1=self._make_layer(block,64,layers[0])\n",
        "      self.LightClassificationBranch1=LightClassificationBranch(64,num_classes=num_classes)\n",
        "      self.layer2=self._make_layer(block,128,layers[1],stride=2)\n",
        "      self.LightClassificationBranch2=LightClassificationBranch(128,num_classes=num_classes)\n",
        "      self.layer3=self._make_layer(block,256,1,stride=2)\n",
        "\n",
        "      self.layer32=self._make_layer(block,256,layers[2])\n",
        "      self.layer42=self._make_layer(block,512,layers[3],stride=2)\n",
        "      self.avgpool2=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc2=nn.Linear(512*block.expansion,3)\n",
        "      \n",
        "      self.inplanes=256\n",
        "      self.layer31=self._make_layer(block2,128,layers[4],stride=2)\n",
        "      self.layer41=self._make_layer(block2,256,layers[5],stride=2)\n",
        "      self.layer51=self._make_layer(block2,512,layers[6],stride=2)\n",
        "      \n",
        "\n",
        "     \n",
        "      self.avgpool1=nn.AdaptiveAvgPool2d((1,1))\n",
        "      \n",
        "      self.fc1=nn.Linear(512*block2.expansion,num_classes)\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "      downsample = None  \n",
        "  \n",
        "      if stride != 1 or self.inplanes != planes*block.expansion:\n",
        "          downsample = nn.Sequential(\n",
        "              nn.Conv2d(self.inplanes, planes*block.expansion, 1, stride, bias=False),\n",
        "              nn.BatchNorm2d(planes*block.expansion),\n",
        "          )\n",
        "\n",
        "      layers = []\n",
        "      layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "      \n",
        "      self.inplanes = planes*block.expansion\n",
        "      \n",
        "      for _ in range(1, blocks):\n",
        "          layers.append(block(self.inplanes, planes))\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=self.conv1(x)\n",
        "      x=self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.maxpool(x)         # 112x112\n",
        "      \n",
        "      x = self.layer1(x)\n",
        "      feature_vector1,out1=self.LightClassificationBranch1(x)          # 56x56\n",
        "      \n",
        "      \n",
        "      x = self.layer2(x)  \n",
        "      feature_vector2,out2=self.LightClassificationBranch2(x)\n",
        "      x = self.layer3(x)          # 14x14\n",
        "      \n",
        "      x1 = self.layer31(x)\n",
        "      x1= self.layer41(x1)          # 7x7\n",
        "      x1=self.layer51(x1)\n",
        "      x1 = self.avgpool1(x1)         # 1x1\n",
        "      x1 = torch.flatten(x1, 1)\n",
        "      #prova=x1+feature_vector1+feature_vector2\n",
        "      x1=torch.stack((x1,feature_vector1,feature_vector2),axis=1)\n",
        "      x1=torch.sum(x1,axis=1)     # remove 1 X 1 grid and make vector of tensor shape \n",
        "      x1 = self.fc1(x1)\n",
        "      \n",
        "      x2= self.layer32(x)\n",
        "      x2=self.layer42(x2)\n",
        "      x2=self.avgpool2(x2)\n",
        "      x2=torch.flatten(x2,1)\n",
        "      \n",
        "      x2=self.fc2(x2)\n",
        "\n",
        "\n",
        "      return out1,out2,x1,x2"
      ],
      "metadata": {
        "id": "Z8E0kLCRwLDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone34withBranches"
      ],
      "metadata": {
        "id": "8kkMhFeA19c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetModmod34(nn.Module):\n",
        "  def __init__(self,block,block2,layers,num_classes=21):\n",
        "      super().__init__()\n",
        "\n",
        "      self.inplanes=64\n",
        "      self.conv1=nn.Conv2d(3,self.inplanes,kernel_size=7,stride=2,padding=3,bias=False)\n",
        "      self.bn1=nn.BatchNorm2d(self.inplanes)\n",
        "      self.relu=nn.ReLU(inplace=True)\n",
        "      self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "      #self.classificationBranch=ClassificationBranch(inplanes=64,planes=128)\n",
        "      \n",
        "      self.layer1=self._make_layer(block,64,layers[0])\n",
        "      self.LightClassificationBranch1=LightClassificationBranch(64,num_classes=num_classes)\n",
        "      \n",
        "      self.layer2=self._make_layer(block,128,layers[1],stride=2)\n",
        "      self.LightClassificationBranch2=LightClassificationBranch(128,num_classes=num_classes)\n",
        "      self.layer3=self._make_layer(block,256,1,stride=2)\n",
        "\n",
        "      self.layer32=self._make_layer(block,256,layers[2])\n",
        "      self.layer42=self._make_layer(block,512,layers[3],stride=2)\n",
        "      self.avgpool2=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc2=nn.Linear(512*block.expansion,3)\n",
        "      \n",
        "      self.inplanes=256\n",
        "      self.layer31=self._make_layer(block2,128,layers[4],stride=2)\n",
        "      self.layer41=self._make_layer(block2,256,layers[5],stride=2)\n",
        "      self.layer51=self._make_layer(block2,512,layers[6],stride=2)\n",
        "      \n",
        "\n",
        "     \n",
        "      self.avgpool1=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc1=nn.Linear(512*block2.expansion,num_classes)\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "      downsample = None  \n",
        "  \n",
        "      if stride != 1 or self.inplanes != planes*block.expansion:\n",
        "          downsample = nn.Sequential(\n",
        "              nn.Conv2d(self.inplanes, planes*block.expansion, 1, stride, bias=False),\n",
        "              nn.BatchNorm2d(planes*block.expansion),\n",
        "          )\n",
        "\n",
        "      layers = []\n",
        "      layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "      \n",
        "      self.inplanes = planes*block.expansion\n",
        "      \n",
        "      for _ in range(1, blocks):\n",
        "          layers.append(block(self.inplanes, planes))\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=self.conv1(x)\n",
        "      x=self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.maxpool(x)         # 112x112\n",
        "      \n",
        "      x = self.layer1(x)  \n",
        "      feature_vector1,out1=self.LightClassificationBranch1(x)        \n",
        "      \n",
        "      x = self.layer2(x) \n",
        "      feature_vector2,out2=self.LightClassificationBranch2(x)         \n",
        "      x = self.layer3(x)          # 14x14\n",
        "      \n",
        "      x1 = self.layer31(x)\n",
        "      x1= self.layer41(x1)          # 7x7\n",
        "      x1=self.layer51(x1)\n",
        "      x1 = self.avgpool1(x1)         # 1x1\n",
        "      x1 = torch.flatten(x1, 1)     # remove 1 X 1 grid and make vector of tensor shape \n",
        "      x1 = self.fc1(x1)\n",
        "      \n",
        "      x2= self.layer32(x)\n",
        "      x2=self.layer42(x2)\n",
        "      x2=self.avgpool2(x2)\n",
        "      x2=torch.flatten(x2,1)\n",
        "      x2=self.fc2(x2)\n",
        "\n",
        "\n",
        "      return out1,out2,x1,x2"
      ],
      "metadata": {
        "id": "OhbyIPOhok2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelmod34=resnetmodmod34()\n",
        "initializeWeights34(modelmod34)\n"
      ],
      "metadata": {
        "id": "mgR3hmGcC558"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=modelmod34(inp)\n",
        "output"
      ],
      "metadata": {
        "id": "vx6ndcSHPObf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=modelmod34.conv1.weight\n",
        "b=resnetpre.conv1.weight\n",
        "torch.equal(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKZSaCqFUB4u",
        "outputId": "a2929792-c392-4547-fa92-fd11e6ace391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "KRFV9JfWNHyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {0: 'baboon',\n",
        "              1: 'buffalo',\n",
        "              2: 'cheetah',\n",
        "              3: 'eland',\n",
        "              4: 'elephant',\n",
        "              5: 'empty',\n",
        "              6: 'gazellegrants',\n",
        "              7: 'gazellethomsons',\n",
        "              8: 'giraffe',\n",
        "              9: 'guineafowl',\n",
        "              10: 'hartebeest',\n",
        "              11: 'hyenaspotted',\n",
        "              12: 'impala',\n",
        "              13: 'koribustard',\n",
        "              14: 'lionfemale',\n",
        "              15: 'lionmale',\n",
        "              16: 'otherbird',\n",
        "              17: 'topi',\n",
        "              18: 'warthog',\n",
        "              19: 'wildebeest',\n",
        "              20: 'zebra'}\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(p=0.3),\n",
        "        transforms.RandomApply(transforms=[transforms.RandomRotation((90,270))],p=0.4),\n",
        "        transforms.RandomApply(transforms=[transforms.RandomAffine(degrees=0,translate=(0.1, 0.1))],p=0.1),\n",
        "        #transforms.Resize(300),\n",
        "        transforms.RandomResizedCrop(224,scale=(0.5,1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'train_empty': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_empty': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "data_dir = 'drive/MyDrive/data/dataset'\n",
        "image_datasets = {x: SerengetiDataset(os.path.join(data_dir, x + '.csv'), os.path.join(data_dir, x),\n",
        "                                      data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "data_dir_empty = 'drive/MyDrive/data/empty_dataset'\n",
        "\n",
        "image_datasets.update({x: datasets.ImageFolder(os.path.join(data_dir_empty, y),\n",
        "                                               data_transforms[x])\n",
        "                       for y, x in zip(['train', 'val'], ['train_empty', 'val_empty'])})\n",
        "\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=64, shuffle=True, pin_memory=True , num_workers=8)\n",
        "               for x in ['train', 'val', 'train_empty', 'val_empty']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'train_empty', 'val_empty']}\n",
        "\n",
        "class_names = list(labels_map.values())\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "SUUY3J2ANMlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=100, patience=10,\n",
        "                early_stopping_based_on_loss=True):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    #best_empty_acc=0.0\n",
        "    best_loss = np.Inf\n",
        "    #epoch_acc_empty=0.0\n",
        "    list_epoch_loss=[]\n",
        "    list_average_epoch_acc=[]\n",
        "    list_running_corrects_specie_first=[]\n",
        "    list_running_corrects_specie_second=[]\n",
        "    list_running_corrects_specie_final=[]\n",
        "    list_running_corrects_majorVoting=[]\n",
        "    list_epoch_acc_descrizione=[]\n",
        "\n",
        "    if early_stopping_based_on_loss:\n",
        "        print('Early stopping based on loss...')\n",
        "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    else:\n",
        "        print('Early stopping based on accuracy...')\n",
        "        early_stopping = EarlyStopping(patience=patience, verbose=True, loss_based=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        # TODO Rimuovi train e rinomina variabili\n",
        "        '''\n",
        "        if (epoch_acc_empty < 0.55 and epoch > 35)  or epoch < 10:\n",
        "                torch.save(model.state_dict(), os.path.join('results/models', 'lastbestweightsofempty.pth'))\n",
        "\n",
        "        if epoch >=10:\n",
        "            if epoch==10:\n",
        "                early_stopping.reset()\n",
        "        '''\n",
        "        for phase in ['train','val']:\n",
        "            if phase in ['train']:\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            #running_corrects = 0\n",
        "            running_corrects_specie_first=0\n",
        "            running_corrects_specie_second = 0\n",
        "            running_corrects_specie_final = 0\n",
        "            running_corrects_majorVoting=0\n",
        "            #running_corrects_empty=0\n",
        "            running_corrects_descrizione=0\n",
        "\n",
        "\n",
        "            # Iterate over data.\n",
        "\n",
        "            for inputs, specie, descrizione, _ in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                specie = specie.to(device)\n",
        "                descrizione = descrizione.to(device)\n",
        "                #emptyimg = emptyimg.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    out_specie_first,out_specie_second,out_specie_final,out_descrizione = model(inputs)\n",
        "\n",
        "                    #_, preds_emptyimg = torch.max(outputs, 1)\n",
        "                    _, preds_specie_first = torch.max(out_specie_first, 1)\n",
        "                    _, preds_specie_second = torch.max(out_specie_second, 1)\n",
        "                    _, preds_specie_final = torch.max(out_specie_final, 1)\n",
        "                    _, preds_descrizione = torch.max(out_descrizione, 1)\n",
        "                    preds_specie_majorVoting=preds_specie_first+preds_specie_second+preds_specie_final\n",
        "\n",
        "                    #loss1 = criterion(outputs, emptyimg)\n",
        "                    loss1 = criterion(out_specie_first, specie)\n",
        "                    loss2 = criterion(out_specie_second, specie)\n",
        "                    loss3 = criterion(out_specie_final, specie)\n",
        "                    loss4 = criterion(out_descrizione, descrizione)\n",
        "\n",
        "                    loss_tot = loss1 + loss2 + loss3+loss4\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss_tot.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss_tot.item() * inputs.size(0)\n",
        "                #if phase == 'val':\n",
        "                running_corrects_specie_first+=torch.sum(preds_specie_first==specie.data)\n",
        "                running_corrects_specie_second += torch.sum(preds_specie_second == specie.data)\n",
        "                running_corrects_specie_final += torch.sum(preds_specie_final == specie.data)\n",
        "                running_corrects_majorVoting+=torch.sum(preds_specie_majorVoting == specie.data)\n",
        "                running_corrects_descrizione+=torch.sum(preds_descrizione == torch.max(descrizione.data, 1).indices)\n",
        "\n",
        "                '''\n",
        "                running_corrects += torch.sum(torch.logical_and(\n",
        "                    torch.logical_and(preds_emptyimg == emptyimg.data, preds_specie == specie.data),\n",
        "                    preds_descrizione == torch.max(descrizione.data, 1).indices))\n",
        "                '''\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc_specie = running_corrects_specie_final.double() / dataset_sizes[phase]\n",
        "            epoch_acc_specie_MajorVoting=running_corrects_majorVoting.double() / dataset_sizes[phase]\n",
        "            epoch_acc_descrizione = running_corrects_descrizione.double() / dataset_sizes[phase]\n",
        "            average_epoch_acc = (epoch_acc_specie_MajorVoting + epoch_acc_descrizione) / 2\n",
        "\n",
        "            if phase == 'train':\n",
        "                list_epoch_loss.append(epoch_loss)\n",
        "\n",
        "            if phase == 'val':\n",
        "                #epoch_acc_specie=running_corrects_specie_final.double() / dataset_sizes[phase]\n",
        "                epoch_acc_specie_first = running_corrects_specie_first.double() / dataset_sizes[phase]\n",
        "                epoch_acc_specie_second = running_corrects_specie_second.double() / dataset_sizes[phase]\n",
        "                #epoch_acc_specie_MajorVoting = running_corrects_majorVoting.double() / dataset_sizes[phase]\n",
        "                list_running_corrects_specie_first.append(epoch_acc_specie_first)\n",
        "                list_running_corrects_specie_second.append(epoch_acc_specie_second)\n",
        "                list_running_corrects_specie_final.append(epoch_acc_specie)\n",
        "                list_running_corrects_majorVoting.append(epoch_acc_specie_MajorVoting)\n",
        "\n",
        "\n",
        "                #epoch_acc_descrizione = running_corrects_descrizione.double() / dataset_sizes[phase]\n",
        "                #average_epoch_acc=(epoch_acc_specie+epoch_acc_descrizione)/2\n",
        "                list_epoch_acc_descrizione.append(epoch_acc_descrizione)\n",
        "                list_average_epoch_acc.append(average_epoch_acc)\n",
        "                print('{} Loss: {:.4f} Acc: {:.4f} Acc_Specie_First: {:.4f} Acc_Specie_Second: {:.4f} Acc_Specie_Final: {:.4f} Acc_Specie_MajorVoting: {:.4f} Acc_Descrizione: {:.4f}'.format(phase, epoch_loss, average_epoch_acc,epoch_acc_specie_first,epoch_acc_specie_second,epoch_acc_specie,epoch_acc_specie_MajorVoting,epoch_acc_descrizione))\n",
        "            else:\n",
        "\n",
        "                print('{} Loss: {:.4f} Acc: {:.4f} Acc_MajorVoting: {:.4f}'.format(phase, epoch_loss, average_epoch_acc,epoch_acc_specie_MajorVoting))\n",
        "\n",
        "            # deep copy the model\n",
        "            if early_stopping_based_on_loss:\n",
        "                if phase == 'val' and epoch_loss < best_loss:\n",
        "                    best_acc = average_epoch_acc\n",
        "                    best_loss = epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    torch.save(model.state_dict(),os.path.join('drive/MyDrive/results/models','lastbestweightsLightBranchesResNet34Backbone.pth'))\n",
        "                    print('Better val loss')\n",
        "            else:\n",
        "                if phase == 'val' and average_epoch_acc > best_acc:\n",
        "                    best_acc = average_epoch_acc\n",
        "                    best_loss = epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    torch.save(model.state_dict(), os.path.join('drive/MyDrive/results/models', 'lastbestweightsLightBranchesResNet34Backbone.pth'))\n",
        "\n",
        "                    print('Better val accuracy')\n",
        "\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "        if early_stopping_based_on_loss:\n",
        "            early_stopping(epoch_loss, model)\n",
        "        else:\n",
        "            early_stopping(average_epoch_acc, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(list_epoch_loss)\n",
        "    print(list_average_epoch_acc)\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Loss: {:4f} with Acc of {:4f}'.format(best_loss, best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    '''\n",
        "    f = open('lossResnet34withBracnhes.txt', 'w')\n",
        "    for ele in list_epoch_loss:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "    f = open('averageEpochAccResnet34withBracnhes.txt', 'w')\n",
        "    for ele in list_average_epoch_acc:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "    f = open('MajorVotingAccResnet34withBracnhes.txt', 'w')\n",
        "    for ele in list_running_corrects_majorVoting:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "    f = open('FirstOutputAccResnet34withBracnhes.txt', 'w')\n",
        "    for ele in list_running_corrects_specie_first:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "    f = open('SecondOutputAccResnet34withBracnhes.txt', 'w')\n",
        "    for ele in list_running_corrects_specie_second:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "    f = open('FinalOutputAccResnet34withBracnhes.txt', 'w')\n",
        "    for ele in list_running_corrects_specie_final:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "    f = open('DescriptionAccResnet34withBracnhes.txt', 'w')\n",
        "    for ele in list_epoch_acc_descrizione:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "    '''\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "geDvj-NjsTMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelmod34.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(modelmod34.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)"
      ],
      "metadata": {
        "id": "Iv-fPbGN9CTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelmod34=train_model(modelmod34, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                           patience=7,num_epochs=60, early_stopping_based_on_loss=False)\n",
        "torch.save(modelmod34, os.path.join('drive/MyDrive/results/models', '34BackboneWithBranches.pth'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "0yByVLnF7UfL",
        "outputId": "5b052d9e-debb-403b-d405-bf83b15f61f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping based on accuracy...\n",
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 31.4524 Acc: 0.2583 Acc_MajorVoting: 0.0111\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a0d51a4734c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m modelmod34=train_model(modelmod34, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                            patience=7,num_epochs=100, early_stopping_based_on_loss=False)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelmod34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/MyDrive/results/models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'34BackboneWithBranches.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-703baca23d13>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs, patience, early_stopping_based_on_loss)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescrizione\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mspecie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 3.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-5-4feb08583647>\", line 13, in __getitem__\n    image = read_image(img_path)\n  File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 223, in read_image\n    return decode_image(data, mode)\n  File \"/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\", line 202, in decode_image\n    output = torch.ops.image.decode_image(input, mode.value)\nRuntimeError: Image is incomplete or truncated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backbone50"
      ],
      "metadata": {
        "id": "5M-WQ2N9uWw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone50withoutBranches"
      ],
      "metadata": {
        "id": "NAXWUKOHQq6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Backbone50(nn.Module):\n",
        "  def __init__(self,block,layers,num_classes=21):\n",
        "      super().__init__()\n",
        "\n",
        "      self.inplanes=64\n",
        "      self.conv1=nn.Conv2d(3,self.inplanes,kernel_size=7,stride=2,padding=3,bias=False)\n",
        "      self.bn1=nn.BatchNorm2d(self.inplanes)\n",
        "      self.relu=nn.ReLU(inplace=True)\n",
        "      self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "      #self.classificationBranch=ClassificationBranch(inplanes=256,planes=128)\n",
        "      \n",
        "      self.layer1=self._make_layer(block,64,layers[0])\n",
        "      #self.LightClassificationBranch1=LightClassificationBranch(inplanes=256,num_classes=num_classes)\n",
        "      \n",
        "      self.layer2=self._make_layer(block,128,layers[1],stride=2)\n",
        "      #self.LightClassificationBranch2=LightClassificationBranch(inplanes=512,num_classes=num_classes)\n",
        "      self.layer3=self._make_layer(block,256,1,stride=2)\n",
        "\n",
        "      self.layer31=self._make_layer(block,256,layers[2])\n",
        "      self.layer41=self._make_layer(block,512,layers[3],stride=2)\n",
        "      self.avgpool1=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc1=nn.Linear(512*block.expansion,num_classes)\n",
        "      self.inplanes=1024\n",
        "      self.layer32=self._make_layer(block,256,layers[2])\n",
        "      self.layer42=self._make_layer(block,512,layers[3],stride=2)\n",
        "      self.avgpool2=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc2=nn.Linear(512*block.expansion,3)\n",
        "\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "      downsample = None  \n",
        "  \n",
        "      if stride != 1 or self.inplanes != planes*block.expansion:\n",
        "          downsample = nn.Sequential(\n",
        "              nn.Conv2d(self.inplanes, planes*block.expansion, 1, stride, bias=False),\n",
        "              nn.BatchNorm2d(planes*block.expansion),\n",
        "          )\n",
        "\n",
        "      layers = []\n",
        "      layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "      \n",
        "      self.inplanes = planes*block.expansion\n",
        "      \n",
        "      for _ in range(1, blocks):\n",
        "          layers.append(block(self.inplanes, planes))\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=self.conv1(x)\n",
        "      x=self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.maxpool(x)         # 112x112\n",
        "      \n",
        "      x = self.layer1(x)          # 56x56\n",
        "      \n",
        "      x = self.layer2(x)   \n",
        "             # 28x28\n",
        "      x = self.layer3(x)          # 14x14\n",
        "      \n",
        "      x1 = self.layer31(x)\n",
        "      x1= self.layer41(x1)          # 7x7\n",
        "      x1 = self.avgpool1(x1)         # 1x1\n",
        "      x1 = torch.flatten(x1, 1)\n",
        "      #x1=torch.stack((x1,feature_vector1,feature_vector2),axis=1)\n",
        "      #x1=torch.sum(x1,axis=1) \n",
        "      x1 = self.fc1(x1)\n",
        "      \n",
        "      x2= self.layer32(x)\n",
        "      x2=self.layer42(x2)\n",
        "      x2=self.avgpool2(x2)\n",
        "      x2=torch.flatten(x2,1)\n",
        "      x2=self.fc2(x2)\n",
        "\n",
        "\n",
        "      return x1,x2"
      ],
      "metadata": {
        "id": "f-GPFTCKQyf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone50=backbone50()\n",
        "backbone50"
      ],
      "metadata": {
        "id": "CRHrusOnTGFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=backbone50.conv1.weight\n",
        "b=resntet50pre.conv1.weight\n",
        "torch.equal(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtP4uAkjrgh-",
        "outputId": "17d966d6-cf0a-449c-fb0a-bf60b3fbe02b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1,x2=backbone50(inp)"
      ],
      "metadata": {
        "id": "sU0iFjY6TS34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initializeWeights50(backbone50)"
      ],
      "metadata": {
        "id": "gyFKtrv5TYj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "A5mswju-TGXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {0: 'baboon',\n",
        "              1: 'buffalo',\n",
        "              2: 'cheetah',\n",
        "              3: 'eland',\n",
        "              4: 'elephant',\n",
        "              5: 'empty',\n",
        "              6: 'gazellegrants',\n",
        "              7: 'gazellethomsons',\n",
        "              8: 'giraffe',\n",
        "              9: 'guineafowl',\n",
        "              10: 'hartebeest',\n",
        "              11: 'hyenaspotted',\n",
        "              12: 'impala',\n",
        "              13: 'koribustard',\n",
        "              14: 'lionfemale',\n",
        "              15: 'lionmale',\n",
        "              16: 'otherbird',\n",
        "              17: 'topi',\n",
        "              18: 'warthog',\n",
        "              19: 'wildebeest',\n",
        "              20: 'zebra'}\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(p=0.3),\n",
        "        T.RandomApply(transforms=[T.RandomRotation((30))], p=0.3),\n",
        "        T.RandomApply(transforms=[T.GaussianBlur(kernel_size=(7), sigma=(1))], p=0.1),\n",
        "        # transforms.Resize(300),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.5, 1)),\n",
        "        transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'train_empty': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(p=0.3),\n",
        "        T.RandomApply(transforms=[T.RandomRotation((30))], p=0.3),\n",
        "        T.RandomApply(transforms=[T.GaussianBlur(kernel_size=(7), sigma=(1))], p=0.1),\n",
        "        # transforms.Resize(300),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.5, 1)),\n",
        "        transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_empty': transforms.Compose([\n",
        "        transforms.Resize(300),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "data_dir = 'drive/MyDrive/data/dataset'\n",
        "image_datasets = {x: SerengetiDataset(os.path.join(data_dir, x + '.csv'), os.path.join(data_dir, x),\n",
        "                                      data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "data_dir_empty = 'drive/MyDrive/data/empty_dataset'\n",
        "\n",
        "image_datasets.update({x: datasets.ImageFolder(os.path.join(data_dir_empty, y),\n",
        "                                               data_transforms[x])\n",
        "                       for y, x in zip(['train', 'val'], ['train_empty', 'val_empty'])})\n",
        "\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=128, shuffle=True, pin_memory=True , num_workers=8)\n",
        "               for x in ['train', 'val', 'train_empty', 'val_empty']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'train_empty', 'val_empty']}\n",
        "\n",
        "class_names = list(labels_map.values())\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "O31-aMifTRld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=100, patience=10,\n",
        "                early_stopping_based_on_loss=False):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    #best_empty_acc=0.0\n",
        "    best_loss = np.Inf\n",
        "    #epoch_acc_empty=0.0\n",
        "    list_epoch_loss=[]\n",
        "    list_average_epoch_acc=[]\n",
        "    #list_running_corrects_specie_first=[]\n",
        "    #list_running_corrects_specie_second=[]\n",
        "    list_epoch_acc_specie=[]\n",
        "\n",
        "    list_epoch_acc_descrizione=[]\n",
        "\n",
        "    if early_stopping_based_on_loss:\n",
        "        print('Early stopping based on loss...')\n",
        "        early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    else:\n",
        "        print('Early stopping based on accuracy...')\n",
        "        early_stopping = EarlyStopping(patience=patience, verbose=True, loss_based=False)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        # TODO Rimuovi train e rinomina variabili\n",
        "        '''\n",
        "        if (epoch_acc_empty < 0.55 and epoch > 35)  or epoch < 10:\n",
        "                torch.save(model.state_dict(), os.path.join('results/models', 'lastbestweightsofempty.pth'))\n",
        "\n",
        "        if epoch >=10:\n",
        "            if epoch==10:\n",
        "                early_stopping.reset()\n",
        "        '''\n",
        "        for phase in ['train','val']:\n",
        "            if phase in ['train']:\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects_specie = 0\n",
        "            running_corrects_descrizione=0\n",
        "\n",
        "\n",
        "            # Iterate over data.\n",
        "\n",
        "            for inputs, specie, descrizione, _ in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                specie = specie.to(device)\n",
        "                descrizione = descrizione.to(device)\n",
        "                #emptyimg = emptyimg.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    out_specie,out_descrizione = model(inputs)\n",
        "\n",
        "                    #_, preds_emptyimg = torch.max(outputs, 1)\n",
        "\n",
        "                    _, preds_specie = torch.max(out_specie, 1)\n",
        "                    _, preds_descrizione = torch.max(out_descrizione, 1)\n",
        "\n",
        "\n",
        "                    #loss1 = criterion(outputs, emptyimg)\n",
        "                    loss1 = criterion(out_specie, specie)\n",
        "\n",
        "                    loss2 = criterion(out_descrizione, descrizione)\n",
        "\n",
        "                    loss_tot = loss1 + loss2\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss_tot.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss_tot.item() * inputs.size(0)\n",
        "                #if phase == 'val':\n",
        "                running_corrects_specie+=torch.sum(preds_specie==specie.data)\n",
        "                running_corrects_descrizione+=torch.sum(preds_descrizione == torch.max(descrizione.data, 1).indices)\n",
        "\n",
        "                '''\n",
        "                running_corrects += torch.sum(torch.logical_and(\n",
        "                    torch.logical_and(preds_emptyimg == emptyimg.data, preds_specie == specie.data),\n",
        "                    preds_descrizione == torch.max(descrizione.data, 1).indices))\n",
        "                '''\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc_specie = running_corrects_specie.double() / dataset_sizes[phase]\n",
        "\n",
        "            epoch_acc_descrizione = running_corrects_descrizione.double() / dataset_sizes[phase]\n",
        "            average_epoch_acc = (epoch_acc_specie + epoch_acc_descrizione) / 2\n",
        "\n",
        "            if phase == 'train':\n",
        "                list_epoch_loss.append(epoch_loss)\n",
        "\n",
        "            if phase == 'val':\n",
        "                #epoch_acc_descrizione = running_corrects_descrizione.double() / dataset_sizes[phase]\n",
        "                #average_epoch_acc=(epoch_acc_specie+epoch_acc_descrizione)/2\n",
        "                list_epoch_acc_specie.append(epoch_acc_specie)\n",
        "                list_epoch_acc_descrizione.append(epoch_acc_descrizione)\n",
        "                list_average_epoch_acc.append(average_epoch_acc)\n",
        "\n",
        "\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f} Acc_Specie: {:.4f} Acc_Descrizione: {:.4f}'.format(phase, epoch_loss, average_epoch_acc,epoch_acc_specie,epoch_acc_descrizione))\n",
        "\n",
        "            # deep copy the model\n",
        "            if early_stopping_based_on_loss:\n",
        "                if phase == 'val' and epoch_loss < best_loss:\n",
        "                    best_acc = average_epoch_acc\n",
        "                    best_loss = epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    torch.save(model.state_dict(),os.path.join('drive/MyDrive/results/models','lastbestweights50Backbone.pth'))\n",
        "                    print('Better val loss')\n",
        "            else:\n",
        "                if phase == 'val' and average_epoch_acc > best_acc:\n",
        "                    best_acc = average_epoch_acc\n",
        "                    best_loss = epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    torch.save(model.state_dict(), os.path.join('drive/MyDrive/results/models', 'lastbestweights50Backbone.pth'))\n",
        "\n",
        "                    print('Better val accuracy')\n",
        "\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "        if early_stopping_based_on_loss:\n",
        "            early_stopping(epoch_loss, model)\n",
        "        else:\n",
        "            early_stopping(average_epoch_acc, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(list_epoch_loss)\n",
        "    print(list_average_epoch_acc)\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Loss: {:4f} with Acc of {:4f}'.format(best_loss, best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    '''\n",
        "    f = open('lossBackbone34.txt', 'w')\n",
        "    for ele in list_epoch_loss:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "    f = open('averageEpochAccBackbone34.txt', 'w')\n",
        "    for ele in list_average_epoch_acc:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    f = open('AccSpecieBackbone34.txt', 'w')\n",
        "    for ele in list_epoch_acc_specie:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "\n",
        "\n",
        "    f = open('DescriptionAccBackbone34.txt', 'w')\n",
        "    for ele in list_epoch_acc_descrizione:\n",
        "        f.write(ele + '\\n')\n",
        "\n",
        "    f.close()\n",
        "    '''\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "bxRgVG8sTfJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone50.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(backbone50.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "JY_SrONJTwk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone50=train_model(backbone50, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                           patience=7,num_epochs=10, early_stopping_based_on_loss=False)\n",
        "torch.save(modelmod34, os.path.join('drive/MyDrive/results/models', '50BackboneWithouthBranches.pth'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "ce93d9f3-4bfb-44fc-fe60-c62dd69ab81a",
        "id": "EGttpnw7UAEw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping based on accuracy...\n",
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 3.5411 Acc: 0.3789 Acc_Specie: 0.1758 Acc_Descrizione: 0.5820\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ca9574c262ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m backbone50=train_model(backbone50, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                            patience=7,num_epochs=10, early_stopping_based_on_loss=False)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelmod34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/MyDrive/results/models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'50BackboneWithouthBranches.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-0f634573d8f4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs, patience, early_stopping_based_on_loss)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescrizione\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mspecie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone50withAddBranches"
      ],
      "metadata": {
        "id": "TpGmfKJFHPfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Backbone50withAddBranches(nn.Module):\n",
        "  def __init__(self,block,layers,num_classes=21):\n",
        "      super().__init__()\n",
        "\n",
        "      self.inplanes=64\n",
        "      self.conv1=nn.Conv2d(3,self.inplanes,kernel_size=7,stride=2,padding=3,bias=False)\n",
        "      self.bn1=nn.BatchNorm2d(self.inplanes)\n",
        "      self.relu=nn.ReLU(inplace=True)\n",
        "      self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "      #self.classificationBranch=ClassificationBranch(inplanes=256,planes=128)\n",
        "      \n",
        "      self.layer1=self._make_layer(block,64,layers[0])\n",
        "      self.LightClassificationBranch1=LightClassificationBranch(inplanes=256,num_classes=num_classes)\n",
        "      \n",
        "      self.layer2=self._make_layer(block,128,layers[1],stride=2)\n",
        "      self.LightClassificationBranch2=LightClassificationBranch(inplanes=512,num_classes=num_classes)\n",
        "      self.layer3=self._make_layer(block,256,1,stride=2)\n",
        "\n",
        "      self.layer31=self._make_layer(block,256,layers[2])\n",
        "      self.layer41=self._make_layer(block,512,layers[3],stride=2)\n",
        "      self.avgpool1=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc1=nn.Linear(512*block.expansion,num_classes)\n",
        "      self.inplanes=1024\n",
        "      self.layer32=self._make_layer(block,256,layers[2])\n",
        "      self.layer42=self._make_layer(block,512,layers[3],stride=2)\n",
        "      self.avgpool2=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc2=nn.Linear(512*block.expansion,3)\n",
        "\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "      downsample = None  \n",
        "  \n",
        "      if stride != 1 or self.inplanes != planes*block.expansion:\n",
        "          downsample = nn.Sequential(\n",
        "              nn.Conv2d(self.inplanes, planes*block.expansion, 1, stride, bias=False),\n",
        "              nn.BatchNorm2d(planes*block.expansion),\n",
        "          )\n",
        "\n",
        "      layers = []\n",
        "      layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "      \n",
        "      self.inplanes = planes*block.expansion\n",
        "      \n",
        "      for _ in range(1, blocks):\n",
        "          layers.append(block(self.inplanes, planes))\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=self.conv1(x)\n",
        "      x=self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.maxpool(x)         # 112x112\n",
        "      \n",
        "      x = self.layer1(x)          # 56x56\n",
        "      featurevector1,out1=self.LightClassificationBranch1(x)\n",
        "      x = self.layer2(x)   \n",
        "      featurevector2,out2=self.LightClassificationBranch2(x)       # 28x28\n",
        "      x = self.layer3(x)          # 14x14\n",
        "      \n",
        "      x1 = self.layer31(x)\n",
        "      x1= self.layer41(x1)          # 7x7\n",
        "      x1 = self.avgpool1(x1)         # 1x1\n",
        "      x1 = torch.flatten(x1, 1)\n",
        "      x1=torch.stack((x1,featurevector1,featurevector2),axis=1)\n",
        "      x1=torch.sum(x1,axis=1)\n",
        "      x1 = self.fc1(x1)\n",
        "      \n",
        "      x2= self.layer32(x)\n",
        "      x2=self.layer42(x2)\n",
        "      x2=self.avgpool2(x2)\n",
        "      x2=torch.flatten(x2,1)\n",
        "      x2=self.fc2(x2)\n",
        "\n",
        "\n",
        "      return out1,out2,x1,x2"
      ],
      "metadata": {
        "id": "xtxW_l1PHXQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone50addbranches=backbone50withAddBranches()\n",
        "backbone50addbranches"
      ],
      "metadata": {
        "id": "kXtLWJsGK65K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out1,out2,x1,x2=backbone50addbranches(inp)"
      ],
      "metadata": {
        "id": "lzAyfTsuL4V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uUd2hz8MdKe",
        "outputId": "589a8efb-8668-45e5-cf88-5245b49df083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone50withBranches"
      ],
      "metadata": {
        "id": "VrEp7ZxuvOH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetMod(nn.Module):\n",
        "  def __init__(self,block,layers,num_classes=21):\n",
        "      super().__init__()\n",
        "\n",
        "      self.inplanes=64\n",
        "      self.conv1=nn.Conv2d(3,self.inplanes,kernel_size=7,stride=2,padding=3,bias=False)\n",
        "      self.bn1=nn.BatchNorm2d(self.inplanes)\n",
        "      self.relu=nn.ReLU(inplace=True)\n",
        "      self.maxpool=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
        "      #self.classificationBranch=ClassificationBranch(inplanes=256,planes=128)\n",
        "      \n",
        "      self.layer1=self._make_layer(block,64,layers[0])\n",
        "      self.LightClassificationBranch1=LightClassificationBranch(inplanes=256,num_classes=num_classes)\n",
        "      \n",
        "      self.layer2=self._make_layer(block,128,layers[1],stride=2)\n",
        "      self.LightClassificationBranch2=LightClassificationBranch(inplanes=512,num_classes=num_classes)\n",
        "      self.layer3=self._make_layer(block,256,1,stride=2)\n",
        "\n",
        "      self.layer31=self._make_layer(block,256,layers[2])\n",
        "      self.layer41=self._make_layer(block,512,layers[3],stride=2)\n",
        "      self.avgpool1=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc1=nn.Linear(512*block.expansion,num_classes)\n",
        "      self.inplanes=1024\n",
        "      self.layer32=self._make_layer(block,256,layers[2])\n",
        "      self.layer42=self._make_layer(block,512,layers[3],stride=2)\n",
        "      self.avgpool2=nn.AdaptiveAvgPool2d((1,1))\n",
        "      self.fc2=nn.Linear(512*block.expansion,3)\n",
        "\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "      downsample = None  \n",
        "  \n",
        "      if stride != 1 or self.inplanes != planes*block.expansion:\n",
        "          downsample = nn.Sequential(\n",
        "              nn.Conv2d(self.inplanes, planes*block.expansion, 1, stride, bias=False),\n",
        "              nn.BatchNorm2d(planes*block.expansion),\n",
        "          )\n",
        "\n",
        "      layers = []\n",
        "      layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "      \n",
        "      self.inplanes = planes*block.expansion\n",
        "      \n",
        "      for _ in range(1, blocks):\n",
        "          layers.append(block(self.inplanes, planes))\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "      x=self.conv1(x)\n",
        "      x=self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.maxpool(x)         # 112x112\n",
        "      \n",
        "      x = self.layer1(x)          # 56x56\n",
        "      featurevector1,out1=self.LightClassificationBranch1(x)\n",
        "      x = self.layer2(x)   \n",
        "      featurevector2,out2=self.LightClassificationBranch2(x)       # 28x28\n",
        "      x = self.layer3(x)          # 14x14\n",
        "      \n",
        "      x1 = self.layer31(x)\n",
        "      x1= self.layer41(x1)          # 7x7\n",
        "      x1 = self.avgpool1(x1)         # 1x1\n",
        "      x1 = torch.flatten(x1, 1)\n",
        "      x1=torch.stack((x1,feature_vector1,feature_vector2),axis=1)\n",
        "      x1=torch.sum(x1,axis=1) \n",
        "      x1 = self.fc1(x1)\n",
        "      \n",
        "      x2= self.layer32(x)\n",
        "      x2=self.layer42(x2)\n",
        "      x2=self.avgpool2(x2)\n",
        "      x2=torch.flatten(x2,1)\n",
        "      x2=self.fc2(x2)\n",
        "\n",
        "\n",
        "      return out1,out2,x1,x2"
      ],
      "metadata": {
        "id": "fQy6nzAOsSoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "modelmod50=resnetmod50()\n",
        "modelmod50"
      ],
      "metadata": {
        "id": "26C62NYJwMQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output=modelmod50(inp)\n",
        "v1,v2,final,desc=modelmod50(inp)\n",
        "desc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x13UV1WOTuO6",
        "outputId": "c05bb35b-1d86-4fdf-b504-6ddcc8188d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metodi per creazioni reti"
      ],
      "metadata": {
        "id": "zA46iRNt0CkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet34():\n",
        "    layers=[3, 4, 6, 3]\n",
        "    model = ResNet(BasicBlock, layers)\n",
        "    return model\n",
        "\n",
        "def resnet50():\n",
        "  layers=[3,4,6,3]\n",
        "  model= ResNet(Bottleneck,layers)\n",
        "  return model\n",
        "\n",
        "def backbone50():\n",
        "  layers=[3,4,5,3]\n",
        "  model= Backbone50(Bottleneck,layers)\n",
        "  return model\n",
        "\n",
        "\n",
        "def resnetmod50():\n",
        "  layers=[3,4,5,3]\n",
        "  model= ResNetMod(Bottleneck,layers)\n",
        "  return model\n",
        "\n",
        "def backbone50withAddBranches():\n",
        "  layers=[3,4,5,3]\n",
        "  model= Backbone50withAddBranches(Bottleneck,layers)\n",
        "  return model  \n",
        "\n",
        "def backbone34():\n",
        "  layers=[3,4,5,3,4,6,3] \n",
        "  model=Backbone34(BasicBlock,Bottleneck,layers)\n",
        "  return model   \n",
        "\n",
        "def resnetmod34():\n",
        "  layers=[3,4,5,3,4,6,3] \n",
        "  model=Backbone34(BasicBlock,Bottleneck,layers)\n",
        "  return model \n",
        "\n",
        "def resnetmodmod34():\n",
        "  layers=[3,4,5,3,4,6,3] \n",
        "  model=ResNetModmod34(BasicBlock,Bottleneck,layers)\n",
        "  return model  \n",
        "\n",
        "def backbone34withAddBranches():\n",
        "  layers=[3,4,5,3,4,6,3]\n",
        "  model=ResNetModmodAdd34(BasicBlock,Bottleneck,layers)\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FA5CBkngZIz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weights Initialization"
      ],
      "metadata": {
        "id": "31MgbCftL8iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "imqt0YjJMFWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resntet50pre=models.resnet50(pretrained=True)\n",
        "resntet50pre\n"
      ],
      "metadata": {
        "id": "SO-Nf_nTTSM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d46a00-227f-4e16-cadd-8a52d048e4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resntet50pre.bn1.state_dict()"
      ],
      "metadata": {
        "id": "mG84SuH1NDgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078f58b4-8bec-4ed9-cc4c-f338163970a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([2.3888e-01, 2.9136e-01, 3.1615e-01, 2.7122e-01, 2.1731e-01, 3.0903e-01,\n",
              "                      2.2937e-01, 2.3086e-01, 2.1129e-01, 2.8054e-01, 1.9923e-01, 3.1894e-01,\n",
              "                      1.7955e-01, 1.1246e-08, 1.9704e-01, 2.0996e-01, 2.4317e-01, 2.1697e-01,\n",
              "                      1.9415e-01, 3.1569e-01, 1.9648e-01, 2.3214e-01, 2.1962e-01, 2.1633e-01,\n",
              "                      2.4357e-01, 2.9683e-01, 2.3852e-01, 2.1162e-01, 1.4492e-01, 2.9388e-01,\n",
              "                      2.2911e-01, 9.2716e-02, 4.3334e-01, 2.0782e-01, 2.7990e-01, 3.5804e-01,\n",
              "                      2.9315e-01, 2.5306e-01, 2.4210e-01, 2.1755e-01, 3.8645e-01, 2.1003e-01,\n",
              "                      3.6805e-01, 3.3724e-01, 5.0826e-01, 1.9341e-01, 2.3914e-01, 2.6652e-01,\n",
              "                      3.9020e-01, 1.9840e-01, 2.1694e-01, 2.6666e-01, 4.9806e-01, 2.3553e-01,\n",
              "                      2.1349e-01, 2.5951e-01, 2.3547e-01, 1.7579e-01, 4.5354e-01, 1.7102e-01,\n",
              "                      2.4903e-01, 2.5148e-01, 3.8020e-01, 1.9665e-01])),\n",
              "             ('bias',\n",
              "              tensor([ 2.2484e-01,  6.0617e-01,  1.2483e-02,  1.3270e-01,  1.8030e-01,\n",
              "                       1.4739e-01,  1.7430e-01,  1.9023e-01,  2.3226e-01,  2.0082e-01,\n",
              "                       1.2834e-01, -2.1285e-01,  1.5065e-01, -3.9217e-08,  2.4985e-01,\n",
              "                       2.0454e-01,  5.4934e-01,  2.1021e-01,  2.2505e-01,  4.6484e-01,\n",
              "                       2.3888e-01,  2.0442e-01,  2.1546e-01,  6.6194e-01,  2.2755e-01,\n",
              "                       6.6069e-01,  2.0587e-01,  1.9292e-01,  1.1195e-01,  3.3785e-01,\n",
              "                       1.2393e-01,  4.1079e-02,  7.7150e-01,  2.6964e-01,  3.3347e-01,\n",
              "                       5.7908e-01,  1.5026e-01,  1.7534e-01,  1.9429e-01,  1.7248e-01,\n",
              "                       8.0577e-01,  2.3693e-01, -4.3369e-01,  8.4813e-01, -3.7857e-01,\n",
              "                       2.4787e-01,  1.8101e-01,  3.2949e-01, -2.8598e-01,  2.2717e-01,\n",
              "                       2.6168e-01,  5.7609e-02, -5.0320e-01,  1.5704e-01,  1.7890e-01,\n",
              "                       2.8114e-01,  4.2167e-01, -9.7650e-02, -3.1231e-01, -2.5637e-02,\n",
              "                       8.8566e-02,  1.8052e-01,  8.3045e-01,  2.5015e-01])),\n",
              "             ('running_mean',\n",
              "              tensor([ 2.4589e-04, -6.6723e-02,  8.1560e-03, -1.1569e-02,  9.1918e-04,\n",
              "                      -3.9417e-03,  6.8244e-05, -1.0278e-04,  6.2920e-05,  1.0156e-02,\n",
              "                      -1.3827e-03,  8.5514e-03, -9.6786e-04, -5.0542e-09, -1.3922e-03,\n",
              "                       9.7684e-04, -4.2287e-03, -4.3058e-04,  8.8580e-04, -5.7929e-02,\n",
              "                       3.4196e-03, -6.9578e-04, -2.2842e-03,  4.3616e-02,  2.6490e-03,\n",
              "                       4.7517e-03, -1.1206e-03, -3.1145e-04,  2.6560e-03,  5.6712e-02,\n",
              "                      -6.8490e-04, -5.3309e-03, -1.4519e-02, -5.2398e-04, -3.5607e-02,\n",
              "                       8.8422e-02,  1.7903e-03,  4.4882e-03,  1.6055e-03, -3.0522e-04,\n",
              "                       2.9796e-03, -1.2307e-03,  7.8966e-03,  5.9528e-03,  9.8027e-03,\n",
              "                      -1.4366e-03,  3.5911e-04, -9.7358e-04, -9.1738e-03,  7.6298e-05,\n",
              "                      -3.3611e-04, -4.8319e-05,  3.6836e-03, -2.1531e-03, -2.1549e-04,\n",
              "                      -8.7561e-07,  3.4184e-02, -3.5004e-03,  1.1910e-02, -1.0551e-03,\n",
              "                       5.2965e-04,  6.0597e-04, -8.4048e-03, -1.6792e-03])),\n",
              "             ('running_var',\n",
              "              tensor([1.0956e+00, 3.8935e+00, 4.8601e+00, 5.0922e+00, 5.9482e-01, 6.5676e+00,\n",
              "                      6.8618e-01, 6.4897e-01, 4.3926e-01, 4.5669e+00, 4.0052e-01, 2.8487e+00,\n",
              "                      2.9018e-01, 2.9161e-12, 2.0027e-01, 5.0347e-01, 1.5794e+00, 5.2107e-01,\n",
              "                      1.8563e-01, 8.1094e+00, 2.4821e-01, 6.3539e-01, 3.5313e-01, 2.2571e+00,\n",
              "                      1.0236e+00, 3.9252e+00, 7.3166e-01, 6.3357e-01, 7.6921e-02, 5.1747e+00,\n",
              "                      1.3232e+00, 5.5709e-02, 1.4234e+01, 1.0767e-01, 8.3864e+00, 1.3844e+01,\n",
              "                      5.6570e+00, 2.8593e+00, 9.8436e-01, 7.5888e-01, 1.0733e+01, 2.2787e-01,\n",
              "                      2.9440e+00, 5.1670e+00, 9.2119e+00, 4.5519e-01, 6.2876e-01, 2.5094e+00,\n",
              "                      1.5883e+00, 2.9337e-01, 4.9738e-01, 1.9022e+00, 4.4529e+00, 6.9012e-01,\n",
              "                      6.8237e-01, 1.9428e+00, 1.2602e+00, 3.6957e-01, 4.8405e+00, 4.0578e-01,\n",
              "                      1.2564e+00, 1.3468e+00, 9.8125e+00, 2.0060e-01])),\n",
              "             ('num_batches_tracked', tensor(0))])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "resnetpre=models.resnet34(pretrained=True)\n",
        "resnetpre\n"
      ],
      "metadata": {
        "id": "EBnTQxi4bT3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1dae07a-6e44-4815-e1ab-dd51ea17b9e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(resnetpre.conv1.state_dict(),'resnet34conv1.pth')\n",
        "torch.save(resnetpre.layer1.state_dict(),'resnet34layer1.pth')\n",
        "torch.save(resnetpre.layer2.state_dict(),'resnet34layer2.pth')\n",
        "torch.save(resnetpre.layer3.state_dict(),'resnet34layer3.pth')\n",
        "torch.save(resnetpre.layer4.state_dict(),'resnet34layer4.pth')\n",
        "torch.save(resnetpre.fc.state_dict(),'resnet34layerfc.pth')\n",
        "torch.save(resntet50pre.conv1.state_dict(),'resnet50conv1.pth')\n",
        "torch.save(resntet50pre.layer1.state_dict(),'resnet50layer1.pth')\n",
        "torch.save(resntet50pre.layer2.state_dict(),'resnet50layer2.pth')\n",
        "torch.save(resntet50pre.layer3.state_dict(),'resnet50layer3.pth')\n",
        "torch.save(resntet50pre.layer4.state_dict(),'resnet50layer4.pth')\n"
      ],
      "metadata": {
        "id": "_NFFxqmaIc-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initializeWeights34(modelmod34):\n",
        "  modelmod34.conv1.load_state_dict(torch.load('resnet34conv1.pth'))\n",
        "  modelmod34.layer1.load_state_dict(torch.load('resnet34layer1.pth'))\n",
        "  modelmod34.layer2.load_state_dict(torch.load('resnet34layer2.pth'))\n",
        "  modelmod34.layer42.load_state_dict(torch.load('resnet34layer4.pth'))\n",
        "  modelmod34.layer31.load_state_dict(torch.load('resnet50layer2.pth'))\n",
        "  modelmod34.layer41.load_state_dict(torch.load('resnet50layer3.pth'))\n",
        "  modelmod34.layer51.load_state_dict(torch.load('resnet50layer4.pth'))\n"
      ],
      "metadata": {
        "id": "n91HdBGmKKwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initializeWeights50(modelmod50):\n",
        "  modelmod50.conv1.load_state_dict(torch.load('resnet50conv1.pth'))\n",
        "  modelmod50.layer1.load_state_dict(torch.load('resnet50layer1.pth'))\n",
        "  modelmod50.layer2.load_state_dict(torch.load('resnet50layer2.pth'))\n",
        "  modelmod50.layer41.load_state_dict(torch.load('resnet50layer4.pth'))\n",
        "  modelmod50.layer42.load_state_dict(torch.load('resnet50layer4.pth'))\n",
        "  "
      ],
      "metadata": {
        "id": "xKtBWIf3TH6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU"
      ],
      "metadata": {
        "id": "MMOH1KApp__8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73af3026-88e2-4c2d-c420-ec33fcb96259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr 21 14:06:25 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    }
  ]
}